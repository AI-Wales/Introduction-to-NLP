{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Using spaCy to pre-process and analyse text\n",
    "\n",
    "We have already looked at NLTK, but other NLP libraries and packages are available. \n",
    "\n",
    "Here are the most common:\n",
    "\n",
    "1. [NLTK](https://www.nltk.org) for text processing\n",
    "2. [spaCy](https://spacy.io) for fast text processing\n",
    "3. [Gensim](https://radimrehurek.com/gensim/) for topic modelling \n",
    "4. [TextBlob](https://textblob.readthedocs.io/en/dev/) for text processing \n",
    "5. [SciKitLearn](http://scikit-learn.org/stable/) for clustering and topic modelling \n",
    "\n",
    "NLTK's main use is in teaching people how to do language processing in its entirety so it is often too bulky or complicated for use in any 'real' programs. However, spaCy is the modern and most hip way of processing text for programs you intend to actually run for more than explaining things to students! All of these packages have their place and you may end up using tools from all of them if you are building a text processing pipeline (although you should be able to do everything with just spaCy others may have more intuitive interfaces for you to program with).\n",
    "\n",
    "For the rest of these notebooks we will use spaCy. This allows us to show the basics but also get an insight into some of the more interesting and useful aspects of NLP (which are for another session!) in a tool that is very much becoming the industry standard tool for these tasks.\n",
    "\n",
    "We will use the default spaCy model and see how it processes text by examining its outputs. We will also develop our own pipleine to use in spaCy. \n",
    "\n",
    "Before you start make sure you have installed spaCy and the `en` model:\n",
    "\n",
    "\n",
    "```\n",
    "conda install -c conda-forge spacy\n",
    "python -m spacy download en\n",
    "```\n",
    "\n",
    "\n",
    "See the [spaCy](https://spacy.io) documents for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "I was lost,\n",
    "lost on the bypass road.\n",
    "Could be worse,\n",
    "I could be turned to toad.\n",
    "Won't you take me back to my hometown?\n",
    "Take me back before I break down.\n",
    "I say you please return me,\n",
    "Will you ever return me?\n",
    "Will you ever return me?\n",
    "Just like Frankie Fontaine\n",
    "Just like Frankie Fontaine\n",
    "I wonder what can I do?\n",
    "I was found\n",
    "riding a unicorn.\n",
    "Could be worse,\n",
    "I could be backwards born\n",
    "Won't you take me back to my hometown?\n",
    "Take me back before I break down.\n",
    "Will you ever return me?\n",
    "Will you ever return me?\n",
    "Will you ever return me?\n",
    "Just like Frankie Fontaine\n",
    "I say you please return me\n",
    "Will you ever return me?\n",
    "Will you ever return me?\n",
    "Just like Frankie Fontaine\n",
    "I wonder what can I do?\n",
    "calm down and then leave me alone\n",
    "calm down and then leave me alone \n",
    "calm down and then leave me alone \n",
    "calm down and then leave me alone\n",
    "I say you please return me\n",
    "Will you ever return me?\n",
    "Will you ever return me?\n",
    "Just like Frankie Fontaine\n",
    "Just like Frankie Fontaine\n",
    "I wonder what can I do?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Start by importing the spaCy English model and applying it to our text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Investigate the outputs\n",
    "\n",
    "spaCy has performed a lot of processing on the text. This includes: \n",
    "\n",
    "* Tokenisation - Segmenting text into words, punctuation marks etc.\n",
    "* Part-of-Speech (POS) tagging - Assigning word types to tokens, like verb or noun.\n",
    "* Dependency Parsing - Assigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.\n",
    "* Lemmatisation - Assigning the base forms of words. For example, the lemma of \"was\" is \"be\", and the lemma of \"rats\" is \"rat\".\n",
    "* Sentence Boundary Detection (SBD) - Finding and segmenting individual sentences.\n",
    "* Named Entity Recognition (NER) - Labelling named \"real-world\" objects, like persons, companies or locations.\n",
    "* Similarity - Comparing words, text spans and documents and how similar they are to each other.\n",
    "* Text Classification - Assigning categories or labels to a whole document, or parts of a document.\n",
    "* Rule-based Matching - Finding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.\n",
    "\n",
    "Lets have a look at some of these and how we can use them.\n",
    "\n",
    "### Tokenisation\n",
    "\n",
    "spaCy has split the text into individual tokens, preserving punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Lemmatisation\n",
    "\n",
    "spaCy has also performed lemmatised the words in the text. You can view these by looking at the ```lemma_``` property rather that the ```text``` property of a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Tagging\n",
    "\n",

    "spaCy has also tagged each token classifying if it is a digit, punctuation, etc. You can access these using the following methods (all of which output a True or False value):\n",
    "\n",
    "\n",
    "| name        | Description | Invocation          |\n",
    "|-------------|-------------|---------------------|\n",
    "| `is_alpha`  | Whether the token consists of alphabetic characters only | `token.text.isalpha()` |\n",
    "| `is_ascii`  | Whether the token contains only ASCII characters | `any(ord(c) >= 128 for c in token.text)` |\n",
    "| `is_digit`  | Does the token consist only of numeric digits? | `token.text.isdigit()` | \n",
    "| `is_lower`  | Are there only lower case digits in the token? | `token.text.islower()` | \n",
    "| `is_upper`  | Opposite of `is_lower()`, but *not* the negation | `token.txt.isupper()` |\n",
    "| `is_title`  | True if the token is in titlecase | `token.text.istitle()` |\n",
    "| `like_url`  | Does the text look like a website address? | |\n",
    "| `like_num`  | Is the text like a number of any length? | |\n",
    "| `like_email` | Does the text look like an email address? | |\n",
    "\n",
    "The final three are left blank so that you can think about how you would text a piece of text for these things using what you have already learnt today in the regular expressions section. There are more such as \n",
    "\n",
    "- `is_punct`\n",
    "- `is_left_punct` and `is_right_punct`\n",
    "- `is_space`\n",
    "- `is_bracket`\n",
    "- `is_quote`\n",
    "- `is_currency`\n",
    "\n",
    "but we will leave these for you to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print([token.is_punct for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Part of Speech Tagging\n",
    "\n",
    "spaCy is able make a prediction of which tag or label most likely applies in this context. We are starting to break the text into different parts of speech - this is very powerful for analysing text and language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_, token.dep_,token.shape_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Parsing\n",
    "\n",
    "spaCy has also performed some Named Entity Recognition and parsing.\n",
    "\n",
    "### Named Entity Recognition (NER)\n",
    "\n",
    "Lets look at what spaCy has labelled as named entities - note that spaces and new line characters are included here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(\"Text {} -> labelled as {} is a space? {}\".format(ent.text,ent.label_, ent.text==\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "After applying the spaCy model, we can now start to process the test in an appropriate manner for our needs. For example, for sentiment analysis we may wish to look at sentence structure and meaning. To do this we will need the parsing information and punctuation. For statistical models, however, we would want to remove any potential noise like punctuation and stop-words. \n",
    "\n",
    "To remove stop words we could:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note that this function returns a Doc only and strips away any ner - so do this before applying ner!\n",
    "\"\"\"\n",
    "def remove_stopwords(doc):\n",
    "    token_pos = [None] \n",
    "    [token_pos.append(t.i) for t in doc if t.is_stop != False]        \n",
    "    doc = Doc(doc.vocab, words=[t.text for i, t in enumerate(doc) if i not in token_pos])\n",
    "    return doc\n",
    "\n",
    "print(remove_stopwords(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(doc):\n",
    "    token_pos = [None] \n",
    "    [token_pos.append(t.i) for t in doc if t.is_stop != False]        \n",
    "    doc2 = Doc(doc.vocab, words=[t.text for i, t in enumerate(doc) if i not in token_pos])\n",
    "    #doc2.ents = [e for i, e in enumerate(doc.ents) if i not in token_pos]\n",
    "    return doc2\n",
    "\n",
    "d = remove_stopwords(doc)\n",
    "print(d)\n",
    "\n",
    "# Note that there are no ents on this - just a pure doc. So run NER after this to get NER values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We can also get over the issue that spaCy is classifying whitespace and new line characters as named entities:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def remove_whitespace_entities(doc):\n",
    "    doc.ents = [ent for ent in doc.ents if (ent.text != ' ') and (ent.text != '\\n')  ]\n",
    "    return doc\n",
    "\n",
    "doc = remove_whitespace_entities(doc)\n",
    "for ent in doc.ents:\n",
    "    print(\"Text {} -> labelled as {}\".format(ent.text,ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Visualising the parsing\n",
    "\n",
    "It is possible to visualise the parsing on using `displaCy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "sentence_spans = list(doc.sents)\n",
    "displacy.render(sentence_spans[2], style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "displacy.render(sentence_spans[6], style='ent', jupyter=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "05-Hometown Unicorn-Using spaCy.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
