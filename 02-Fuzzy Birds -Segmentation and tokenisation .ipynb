{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Splitting the text into smaller more meaningful chunks\n",
    "\n",
    "One key task in dealing with text is to split it up into smaller, more meaningful chunks that can be processed. \n",
    "\n",
    "Text segmentation and tokenisation are key tools to achieve this. They involve the process of dividing written text into meaningful units, such as words, keywords, phrases, symbols, sentences and other elements called tokens.\n",
    "\n",
    "For this exercise we will look at some of the simplest tokens to understand - line segmentation and word tokenisation.\n",
    "\n",
    "For simplicity lets tokenise by splitting a text into individual words. \n",
    "\n",
    "Let's take a body of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "text_o = \"\"\"Hamster, turnin' round in your wheel\n",
    "I've got something to tell you:\n",
    "I can  harness your  feel\n",
    "Dynamo for electricity\n",
    "You empower my feelings\n",
    "Give me light for me to see\n",
    "(Stavros:)\n",
    "So won't you give   me some food?\n",
    "I really need to get some energy in me\n",
    "Give me some\n",
    "I really need to get some energy in me\n",
    "Give me some\n",
    "The world is turning, kicking, screaming 'round my little head\n",
    "To spread it on my bread\n",
    "Don't you know that I spread it on my bread?\n",
    "(Bunf:)\n",
    "Rest yourself you've been long on your feet\n",
    "If I buy you a matchbox\n",
    "Will you keep it all neat?\n",
    "Radiate, you're no flash in the pan\n",
    "You're the battery incarnate\n",
    "I'm an innocent man\n",
    "(Stavros:)\n",
    "So won't you  give me some food?\n",
    "I really need to  get some energy in me\n",
    "Give me some\n",
    "I really need to get some energy in me\n",
    "Give me some\n",
    "The world is turning, kicking,   screaming 'round my little head\n",
    "To spread it on my bread\n",
    "Don't you know that     I spread it on my bread?\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Word Tokenisation\n",
    "\n",
    "### Breaking down the job - creating a process pipeline\n",
    "\n",
    "Lets split this up into tasks \n",
    "\n",
    "1. remove any punctuation\n",
    "2. we have some text in brackets - lets remove these\n",
    "3. remove any excess white space\n",
    "4. split into a list by whitespace\n",
    "\n",
    "We have the start of a process here  - several simple steps that can be run in sequence to achieve our goal. This is often referred to as a **pipeline**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#remove any punctuation in the text\n",
    "text_processed = re.sub(r'[.,?:\\']', \"\", text_o)\n",
    "\n",
    "#remove the words in brackets - matcxh opening bracket\n",
    "#then any occurance of anything except close bracket\n",
    "text_processed = re.sub(r'\\([^)]*\\)', \"\", text_processed)\n",
    "\n",
    "# remove any excess whitespace\n",
    "text_processed = re.sub( r'  +', \" \", text_processed)\n",
    "\n",
    "print(text_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Split into word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#replace any newline character with a single space\n",
    "textn = re.sub(r'\\n', \" \", text_processed)\n",
    "textn = re.sub( r'  +', \" \", textn)\n",
    "\n",
    "#split on the spaces\n",
    "tokens = textn.split(\" \")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Line Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "To perfrom line segmentatisation we simply split the text up on the newline character '\\n'. We can firstly remove any punctuation and excess whitespace and words in brackets - we can use the processed text output from earlier for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "segments = text_processed.split('\\n')\n",
    "print(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Taking it further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "You can see that we can start to build up lots of small tasks into pipelines to get the text in a format ready for subsequent analysis. Here we have simply split into words and lines, but you can also start splitting into phrases to try and understand the context and meaning of the text in more detail. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "02-Fuzzy Birds -Segmentation and tokenisation .ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
