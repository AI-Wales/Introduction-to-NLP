{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Reducing words to their meaning\n",
    "## Stemming and Lemmatisation\n",
    "\n",
    "So far we have tidied up our text and split it into smaller chunks and removed any frequent joining words. Now we have to start digging into the nature of language and words a little more. One task is to reduce words down to a more basic form, e.g., waiting, waited and waits can be reduced to wait.\n",
    "\n",
    "\n",
    "Question: Should they be? What benefits may we gain from not doing this?\n",
    "\n",
    "\n",
    "This is where things get interesting - and tricky!\n",
    "\n",
    "\n",
    "### What is the difference? \n",
    "Lets take the example of **stemming** - the process of reducing inflected (or sometimes derived) words to their word *stem*; that is, their base or root form. For example, the words; argue, argued, argues, arguing reduce to the stem argu. Usually stemming is a crude heuristic process that chops off the ends of words in the hope of achieving the root correctly most of the time. \n",
    "\n",
    "**Lemmatisation** aims to do this using vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the *lemma*. Most lemmatisers achieve this using a lookup table and so this process, when you have large volumes of text may be slower than stemming. However, if it is a suitable application for your data then it lemmatising is generally the recommended approach to take. This is why the spaCy python module only includes a lemmatiser and no stemming functions.\n",
    "\n",
    "If confronted with the token 'saw', stemming might return just 's', whereas lemmatisation would attempt to return either 'see' or 'saw' depending on whether the use of the token was as a verb or a noun. However, most stemming algorithms impose a 'shortest length' on the tokens they are trying to stem so we should get saw in this instance unless we write out own and its very crude.\n",
    "\n",
    "\n",
    "### Implementing \n",
    "Now we could start to build our own functions here, using rules such as\n",
    "\n",
    "* if the word ends in 'ed', remove the 'ed'\n",
    "\n",
    "* if the word ends in 'ing', remove the 'ing'\n",
    "\n",
    "* if the word ends in 'ly', remove the 'ly'\n",
    "\n",
    "This might work for stemming but lemmatising is a far more complex challenge as you have to generate a whole database of the english language which understands word morphology.\n",
    "\n",
    "\n",
    "But there is good news - someone has already done all the hard work for us! Using existing libraries like [**nltk**](https://www.nltk.org) we can perform stemming and lemmatising quickly and easily.\n",
    "\n",
    "\n",
    "Guess what - they can also be used to tokenise your text too! Lets see how we might use NLTK for this:\n",
    "\n",
    "\n",
    "## Stemming with nltk\n",
    "\n",
    "Nltk has a number of stemming algorithms but Porter (PorterStemmer) is the most popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "words = [\"game\",\"gaming\",\"gamed\",\"games\"]\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "for word in words:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Lemmatising with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for word in words:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Using a larger body of text \n",
    "### Using nltk to tokenise\n",
    "\n",
    "Lets use nltk to tokenise before we apply the lemtatisation and stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "text_o = \"\"\"\n",
    "Locked in a sorry dream\n",
    "You know we're drowning in designer ice creams\n",
    "I scream, this is the present but it's no surprise\n",
    "Then I realize\n",
    "What I see I spies\n",
    "(Nice to see you, to see you, to see you)\n",
    "The past was eagle-eyed\n",
    "(To see you, nice to see you, to see you, to see you)\n",
    "The future's pixelised\n",
    "(To see you, nice to see you, to see you, to see you, to see you)\n",
    "I had my Frisbee sharpened and honed\n",
    "I had it galvanized and chromed\n",
    "Decapitate and bury your toys\n",
    "My Frisbee brings the noise\n",
    "What I see I spies\n",
    "(Nice to see you, to see you, to see you)\n",
    "The past was eagle-eyed\n",
    "(To see you, nice to see you, to see you, to see you)\n",
    "The future's pixelised\n",
    "(To see you, nice to see you, to see you, to see you, to see you)\n",
    "To see you, to see you, to see you, nice to see you\n",
    "(Wipe your windscreen with a chocolate cake)\n",
    "To see you, to see you, to see you, nice to see you\n",
    "(Count your pizzas before they bake)\n",
    "To see you, to see you, to see you, nice to see you\n",
    "(Wipe your windscreen with a chocolate cake)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#tokensise the text\n",
    "tokens = word_tokenize(text_o)\n",
    "\n",
    "# a little bit of additional text to explain the outputs\n",
    "print(\"\\n\\nword    \\t    lemma     \\t    stem\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for token in tokens:\n",
    "    print(f\"{token:10} \\t -> {lemmatiser.lemmatize(token):10} \\t -> {stemmer.stem(token):10}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### What other preprocessing would be useful here? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Bonus - POS with nltk\n",
    "\n",
    "The **part of speech (POS)** tag explains how a word is used in a sentence. There are eight main parts of speech - nouns, pronouns, adjectives, verbs, adverbs, prepositions, conjunctions and interjections. Want to know more - [read here](https://medium.com/greyatom/learning-pos-tagging-chunking-in-nlp-85f7f811a8cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "\n",
    "tokens_pos = pos_tag(tokens)\n",
    "\n",
    "print(tokens_pos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "04-Frisbee-Stemming and lemmatisation.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
