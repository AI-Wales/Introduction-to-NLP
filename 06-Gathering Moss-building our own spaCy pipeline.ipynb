{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing our own spaCy pipeline\n",
    "\n",
    "The basic spaCy modell comes with a default pipeline - it perfomes a series of processing tasks in sequence to give us an output. We can alter and customise that pipeline for our own needs.\n",
    "\n",
    "\n",
    "Before you start make sure you have installed spaCy and the en model:\n",
    "<br>\n",
    "```conda install -c conda-forge spacy```\n",
    "<br>\n",
    "```python -m spacy download en```\n",
    "<br>\n",
    "\n",
    "See the [spaCy](https://spacy.io) documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Lost capacity\n",
    "For infinity\n",
    "You and me are idly\n",
    "Gathering moss\n",
    "Fields of industry\n",
    "Death by misery\n",
    "You and I are idly\n",
    "You and me are idly\n",
    "Gathering moss\n",
    "False security\n",
    "Faked insanity\n",
    "You and I united by\n",
    "Itemised bills\n",
    "Kills my sympathy\n",
    "Builds my agony\n",
    "You and me are idly\n",
    "You and I are idly\n",
    "Gathering moss\n",
    "But when you see me\n",
    "I'll be idly sweeping\n",
    "The dust away\n",
    "(Dust away)\n",
    "Look here comes the shore again\n",
    "And we'll aim there to be merry\n",
    "You and I are idly\n",
    "You and I are idly\n",
    "You and me\n",
    "Gathering moss\n",
    "You and me\n",
    "Gathering moss\n",
    "You and me are idly\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "When we apply the SpaCy model using ```nlp(text)``` spaCy is actually applying several processing steps in series. Each process is applied in turn and the oputput sent to the next process. This is termed a processing pipleine. \n",
    "\n",
    "Instead of applying functions after applying the spaCy model, we can also add them to the default spaCy pipeline. In that was we can develop our own custom spaCy pipelines for our needs. This is a really cool and tidy way of processing our text.\n",
    "\n",
    "Lets explore this - lets start by loading the default model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_default = spacy.load('en')\n",
    "nlp_custom = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The default pipeline\n",
    "We can view the default pipeline is by calling ```pipeline```. We can see that by default SpaCy applies a tagger, a dependency parser and then a entity recoignizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.Tagger at 0x11f2dde10>),\n",
       " ('parser', <spacy.pipeline.DependencyParser at 0x109cb90a0>),\n",
       " ('ner', <spacy.pipeline.EntityRecognizer at 0x109cb9258>)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_custom.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove any process that we dont want. This can speed up performance espcially on very large bodies of text. For example, if we only intend to do statistical analysis and do not need dependency parsing we can remove it from the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customising the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can remove any process that we dont want that can be time consuming but arent required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.Tagger at 0x11f2dde10>),\n",
       " ('ner', <spacy.pipeline.EntityRecognizer at 0x109cb9258>)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_custom.remove_pipe('parser')\n",
    "nlp_custom.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add our own functions to the pipeline.\n",
    "NB - run remove stopwords BEFORE ner - so we perfrom the NER on the reduced body of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note that this function returns a Doc only and strips away any ner - so do this before applying ner!\n",
    "\n",
    "\"\"\"\n",
    "def remove_stopwords(doc):\n",
    "    token_pos = [None] \n",
    "    [token_pos.append(t.i) for t in doc if t.is_stop != False]        \n",
    "    doc = Doc(doc.vocab, words=[t.text for i, t in enumerate(doc) if i not in token_pos])\n",
    "    return doc\n",
    "\n",
    "def remove_whitespace_entities(doc):\n",
    "    doc.ents = [ent for ent in doc.ents if (ent.text != ' ') and (ent.text != '\\n')  ]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.Tagger at 0x11f2dde10>),\n",
       " ('rem_stpw', <function __main__.remove_stopwords(doc)>),\n",
       " ('ner', <spacy.pipeline.EntityRecognizer at 0x109cb9258>),\n",
       " ('rem_ws_ent', <function __main__.remove_whitespace_entities(doc)>)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_custom.add_pipe(remove_stopwords, name='rem_stpw', after = 'tagger')\n",
    "nlp_custom.add_pipe(remove_whitespace_entities, name='rem_ws_ent', after = 'ner')\n",
    "\n",
    "nlp_custom.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the spaCy model now also removes stopwords and whistespaces from the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lost capacity\n",
      "For infinity\n",
      "You and me are idly\n",
      "Gathering moss\n",
      "Fields of industry\n",
      "Death by misery\n",
      "You and I are idly\n",
      "You and me are idly\n",
      "Gathering moss\n",
      "False security\n",
      "Faked insanity\n",
      "You and I united by\n",
      "Itemised bills\n",
      "Kills my sympathy\n",
      "Builds my agony\n",
      "You and me are idly\n",
      "You and I are idly\n",
      "Gathering moss\n",
      "But when you see me\n",
      "I'll be idly sweeping\n",
      "The dust away\n",
      "(Dust away)\n",
      "Look here comes the shore again\n",
      "And we'll aim there to be merry\n",
      "You and I are idly\n",
      "You and I are idly\n",
      "You and me\n",
      "Gathering moss\n",
      "You and me\n",
      "Gathering moss\n",
      "You and me are idly\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_default(text)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Lost capacity \n",
      " For infinity \n",
      " You idly \n",
      " Gathering moss \n",
      " Fields industry \n",
      " Death misery \n",
      " You I idly \n",
      " You idly \n",
      " Gathering moss \n",
      " False security \n",
      " Faked insanity \n",
      " You I united \n",
      " Itemised bills \n",
      " Kills sympathy \n",
      " Builds agony \n",
      " You idly \n",
      " You I idly \n",
      " Gathering moss \n",
      " But \n",
      " I 'll idly sweeping \n",
      " The dust away \n",
      " ( Dust away ) \n",
      " Look comes shore \n",
      " And 'll aim merry \n",
      " You I idly \n",
      " You I idly \n",
      " You \n",
      " Gathering moss \n",
      " You \n",
      " Gathering moss \n",
      " You idly \n",
      " \n"
     ]
    }
   ],
   "source": [
    "doc_pp = nlp_custom(text)\n",
    "print(doc_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Gathering moss \n",
      " Fields industry \n",
      " Death misery \n",
      " You I idly, Gathering moss \n",
      " False, Kills, Builds, Gathering, \n",
      " You \n",
      " Gathering moss)\n"
     ]
    }
   ],
   "source": [
    "print(doc_pp.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This can be saved and used in the future for common text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
